{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to Pinjected\n",
    "\n",
    "Pinjected has multiple features that are individually useful to start with.\n",
    "1. Dependency Injection Concepts\n",
    "    - What is Dependency Injection?\n",
    "    - Constructor Injection\n",
    "    - `Design` object and object creation.\n",
    "        - Design\n",
    "        - Injected\n",
    "        - Resolver\n",
    "    - Decorators to create `Design` object\n",
    "2. Advanced Topics\n",
    "    - Proxied Variables\n",
    "3. Entrypoint Functionalities\n",
    "4. IDE Supports"
   ],
   "id": "abc81b42e86bbdb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# What is Dependency Injection?",
   "id": "54c166b61e5850c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1fc1ef03af812af4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Example with CIFAR-10\n",
    "We describe how a typical machine learning code can be written with pinjected to increase modularity of the code.\n",
    "First we show an original pytorch tutorial script to train and test a model with CIFAR-10 dataset."
   ],
   "id": "5693880a49e8e0de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ],
   "id": "51bfdbd54f555f00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, a simple neural network is trained and tested with CIFAR-10 dataset.\n",
    "This gets the job done, however, we will surely need some more flexibility in the future.\n",
    "For example, we might want to change the neural network architecture or optimizer or loss function.\n",
    "\n",
    "A typical approach for introducing a config object and pass it around everywhere, which we do not recommend.\n",
    "Let's rewrite the code to use config object, as typically seen in many machine learning repositories.\n"
   ],
   "id": "e35a4c253a6c9ab2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Typical ML code with config loading",
   "id": "3b7b39f30d03d893"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```json\n",
    "{\n",
    "  \"data\": {\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    \"batch_size\": 4,\n",
    "    \"num_workers\": 2,\n",
    "    \"root\": \"./data\"\n",
    "  },\n",
    "  \"model\": {\n",
    "    \"name\": \"SimpleCNN\",\n",
    "    \"params\": {\n",
    "      \"conv1_out_channels\": 6,\n",
    "      \"conv1_kernel_size\": 5,\n",
    "      \"conv2_out_channels\": 16,\n",
    "      \"conv2_kernel_size\": 5,\n",
    "      \"fc1_out_features\": 120,\n",
    "      \"fc2_out_features\": 84,\n",
    "      \"num_classes\": 10\n",
    "    }\n",
    "  },\n",
    "  \"training\": {\n",
    "    \"epochs\": 2,\n",
    "    \"loss\": \"CrossEntropyLoss\",\n",
    "    \"optimizer\": {\n",
    "      \"name\": \"SGD\",\n",
    "      \"params\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"momentum\": 0.9\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"paths\": {\n",
    "    \"save_model\": \"./cifar_net.pth\"\n",
    "  }\n",
    "}\n",
    "```"
   ],
   "id": "1fddbb9274b755c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T12:55:58.739961Z",
     "start_time": "2024-10-03T12:55:58.738320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# Load configuration.\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Select dataset\n",
    "dataset_name = config['data']['dataset']\n",
    "if dataset_name == \"CIFAR10\":\n",
    "    trainset = torchvision.datasets.CIFAR10(root=config['data']['root'], train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root=config['data']['root'], train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    num_classes = 10\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=config['data']['batch_size'],\n",
    "                                          shuffle=True, num_workers=config['data']['num_workers'])\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=config['data']['batch_size'],\n",
    "                                         shuffle=False, num_workers=config['data']['num_workers'])\n",
    "\n",
    "\n",
    "# Define models\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, model_config['conv1_out_channels'], model_config['conv1_kernel_size'])\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(model_config['conv1_out_channels'], model_config['conv2_out_channels'],\n",
    "                               model_config['conv2_kernel_size'])\n",
    "        self.fc1 = nn.Linear(model_config['conv2_out_channels'] * 5 * 5, model_config['fc1_out_features'])\n",
    "        self.fc2 = nn.Linear(model_config['fc1_out_features'], model_config['fc2_out_features'])\n",
    "        self.fc3 = nn.Linear(model_config['fc2_out_features'], model_config['num_classes'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Select model\n",
    "model_name = config['model']['name']\n",
    "if model_name == \"SimpleCNN\":\n",
    "    net = SimpleCNN(config['model']['params'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "# Select loss function\n",
    "loss_name = config['training']['loss']\n",
    "if loss_name == \"CrossEntropyLoss\":\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss function: {loss_name}\")\n",
    "\n",
    "# Select optimizer\n",
    "optimizer_config = config['training']['optimizer']\n",
    "optimizer_name = optimizer_config['name']\n",
    "if optimizer_name == \"SGD\":\n",
    "    optimizer = optim.SGD(net.parameters(), **optimizer_config['params'])\n",
    "elif optimizer_name == \"Adam\":\n",
    "    optimizer = optim.Adam(net.parameters(), **optimizer_config['params'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config['training']['epochs']):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "PATH = config['paths']['save_model']\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# Test the network\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "\n",
    "net = SimpleCNN(config['model']['params'])\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ],
   "id": "9abc51aa214f981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now this is the code we typically see in ML community repositories.\n",
    "The code is now configurable to use other models and datasets.\n",
    "\n",
    "However, notice the code is now contaminated with configuration loading with many 'if' statements.\n",
    "The code is now hard to grance over and understand the main logic due to the code mixed with configuration loading.\n",
    "\n",
    "The bad thing is that it is now hard to modify the code for other purposes.\n",
    "Suppose if you want to evaluate the model without training, we need to rewrite the initialization process to only initialize what is needed. In this case, training related stuff such as optimizer and loss function, as well as their associated configuration are not needed and we want to strip it off for performance and readability.\n",
    "\n",
    "This is a tiresome work to do and introduces a lot of redundant code.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "6ae211b15e0fd47d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pinjected version",
   "id": "651790c3739b7d49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "from pinjected import *\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "@instance\n",
    "def transform_default():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Dataset providers\n",
    "@injected\n",
    "def get_cifar10(data_root, transform, /, train=True):\n",
    "    return torchvision.datasets.CIFAR10(root=data_root, train=train,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "\n",
    "@injected\n",
    "def get_dataloader(dataset, batch_size, num_workers, /, shuffle=True):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                       shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, conv1_out_channels, conv1_kernel_size, conv2_out_channels,\n",
    "                 conv2_kernel_size, fc1_out_features, fc2_out_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, conv1_out_channels, conv1_kernel_size)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(conv1_out_channels, conv2_out_channels, conv2_kernel_size)\n",
    "        self.fc1 = nn.Linear(conv2_out_channels * 5 * 5, fc1_out_features)\n",
    "        self.fc2 = nn.Linear(fc1_out_features, fc2_out_features)\n",
    "        self.fc3 = nn.Linear(fc2_out_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Loss function provider\n",
    "@instance\n",
    "def criterion__cross_entropy():\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "@instance\n",
    "def optimizer_sgd(model, learning_rate, momentum=0.9):\n",
    "    return optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "\n",
    "@instance\n",
    "def optimizer_adam(model, learning_rate):\n",
    "    return optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training function\n",
    "@injected\n",
    "def train(model, trainloader, criterion, optimizer, epochs, /):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "# Testing function\n",
    "@injected\n",
    "def test(model, testloader, classes, /):\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                                  for j in range(4)))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "\n",
    "# Main execution function\n",
    "@injected\n",
    "def experiment(train, test,/):\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "run_experiment = experiment()    \n",
    "\n",
    "# Configuration\n",
    "setup_1:Design = design(\n",
    "    data_root='./data',\n",
    "    batch_size=4,\n",
    "    num_workers=2,\n",
    "    model=injected(SimpleCNN)(\n",
    "        conv1_out_channels=6,\n",
    "        conv1_kernel_size=5,\n",
    "        conv2_out_channels=16,\n",
    "        conv2_kernel_size=5,\n",
    "        fc1_out_features=120,\n",
    "        fc2_out_features=84,\n",
    "        num_classes=10\n",
    "    ),\n",
    "    learning_rate=0.001,\n",
    "    epochs=2,\n",
    "    classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'),\n",
    "    transform=transform_default,\n",
    "    trainset=get_cifar10(train=True),\n",
    "    testset=get_cifar10(train=False),\n",
    "    trainloader=get_dataloader(injected('trainset'), shuffle=True),\n",
    "    testloader=get_dataloader(injected('testset'), shuffle=False),\n",
    "    criterion=criterion__cross_entropy,\n",
    "    optimizer=optimizer_sgd\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "if __name__ == \"__main__\":\n",
    "    setup_1.provide(experiment())"
   ],
   "id": "1956614ab22e8d1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, the main difference is that we have `@instance` and `injected` everywhere. Also, the wiring of object dependencies are specified in `setup_1` object.\n",
    "The actual logic runs when we call `provide` method on `setup_1` object. The `provide` method creates all the required objects and wire them together to run the given `experiment` object.\n",
    "\n",
    "Notice that the code has no `if` statement at all to handle configuration loading? This is because the configuration is now handled by `Design` object. This makes us possible to swap any objects with other objects easily.\n",
    "Also, no configuration object is passed around everywhere, and we can clearly see the explicit dependencies required for each object.\n",
    "\n",
    "The design instantiates only the required objects upon calling `provide` method.\n",
    "We can now modify this code to run only the evaluation part, without rewriting whole initialization process.\n",
    "Here is the example:\n",
    "\n",
    "\n"
   ],
   "id": "b6e17d79f512420b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "setup_1.provide(test())",
   "id": "9d611175dbdcd021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the Design object `setup_1` will automatically determine what must be initialized how, and runs the requested `test()`\n",
    "\n",
    "Pinjected allows you to break down the code into modular parts and run only what is needed by automatically wiring them.\n",
    "If you want to take a look at a model architecture, you can do `setup_1.provide('model')`. No other stuff such as training data loading or database initialization will be done, as long as it is not required for instantiating the model!\n",
    "\n",
    "\n"
   ],
   "id": "ec311893c73b345a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "445902ca0fa66be9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "setup_1.provide('model')\n",
   "id": "4bd8aaebabf44fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "`@injected` and `@instance` makes sure to state what is needed for an object to be created by name. upon instantiation of annotated object, the dependencies are looked up by name inside the Design object, and resolved recursively.\n",
    "\n",
    "For `@injected`, the name of function parameters before `/` is treated as its dependencies.\n",
    "For `@instance`, all the name of parameters are treated as its dependencies.\n",
    "\n",
    "Now, how do we change anything registered with `setup_1`?\n",
    "\n",
    "Here an example to change the optimizer to Adam from SGD:\n",
    "\n",
    "\n"
   ],
   "id": "ab6828d2fea746b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "setup_2 = setup_1 + design(\n",
    "    optimizer=optimizer_adam\n",
    ")\n",
    "\n",
    "setup_2.provide('optimizer')"
   ],
   "id": "d34bd54314a5ba1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Additionally, we have a CLI to run the code based on pinjected.\n",
    "\n",
    "```bash\n",
    "python -m pinjected run <module.name_of_target> <module.name_of_design>\n",
    "# can be simplified to;\n",
    "pinjected run <module.name_of_target> <module.name_of_design>\n",
    "```\n",
    "So, if you save the previous training code as `train.py`, you can run the training code with the following command:\n",
    "\n",
    "```bash \n",
    "pinjected run train.run_experiment train.setup_1\n",
    "# or if you want to use adam,\n",
    "pinjected run train.run_experiment train.setup_2\n",
    "# you can also override other stuff:\n",
    "pinjected run train.run_experiment train.setup_1 --epochs=10 --batch_size=10\n",
    "# you can even override with pinjected object, by surrounding its path with '{}'\n",
    "pinjected run train.run_experiment train.setup_1 --optimizer={train.optimizer_sgd}\n",
    "```\n",
    "\n",
    "\n"
   ],
   "id": "30857d8813a244a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a2c370ec34e039d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7a55bc95ac56c651"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5260e55c2f49a36a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
